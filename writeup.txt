Programming for Cognitive Science and AI - Final Project
Ben Hu
Beverly Sihsobhon

Path of Exile (PoE) is an online, multiplayer game that features an online trading economy.  Trading utilizes the in-game items and does not allow real money transactions. Players can collect various game items and store them in “stash tabs” which are recorded by the game servers and can be used or traded with other players.

PoE offers a public stash API, which allows developers to query PoE servers to see the public tabs of players where items are stored. This API is most commonly used to create trade indexing sites where players can list items for trading and also find other player items to trade for. The API documentation can be found at: https://www.pathofexile.com/developer/docs/api-resource-public-stash-tabs and provides an example JSON output as well.
 
Our project uses the Requests Python package to query the API and collect a JSON of the current status of public stash tabs. No player names or account names are included in this data.  We hope to determine the best pricing of item based on the features we have selected.  Knowing proper pricing is useful from both a buyer and seller perspective.  Currently the 3rd party sites available show pricing of an item over time or the average pricing at the moment for each item.  Unfortunately those sites do not have an API, so we use the game developers’ API.
 
Path of Exile's trading system is similar to a bartering economic system in that there is no actual currency, but players can trade items for other items, or materials used to craft items. The most commonly traded item is one of these crafting items, the chaos orb. Because of its ubiquity as a traded commodity, chaos is often referred to as currency so we have chosen the chaos value of each item as the dependent variable. Players who list items for non-chaos items will have their items revalued at an estimated rate to chaos. This rate estimate can be found here: http://currency.poe.trade/. This puts all items on the same measurement scale as players can trade items for any other item, but each item's “value” is often estimated in the number of chaos orbs it can be traded for.
 
The models we have chosen for this project are the K Nearest Neighbor similarity model, the Random Forest model, and the Kernel Support Vector Machine with Linear Kernel.  We predict that the target function is most likely not linear, so using nonlinear models or models like SVM that can better handle non separable data will yield better results.  We use a linear kernel because we do not have enough data for RBF.  However, if we were able to get more data, we would replace the linear kernel with an RBF kernel.
 
There are two important parts to an item in PoE. Each item has some number of affixes, which are modifiers given to the item, and each affix has some range of values that are randomly given to it when it is created. The set of affixes possible for an item varies by the class of the item, which makes a model that predicts prices for all item classes difficult, since the class determines the affixes which are also predictors (More on item affixes can be found here: https://pathofexile.gamepedia.com/Item_affix). For this reason, we are focusing on a particular subset of items, the weapon item class. Within the weapon item class, the affix pool is fixed which makes the traits of each weapon item consistent. Along with the weapon class, we will be excluding items that have the unique rarity, as items with unique rarity have fixed affixes but variable affix values. This makes the price for a particular unique item have low variance, but the price between unique items have extreme variance. Therefore we will be focusing on just items from the other 3 levels of rarity, which have randomly given affixes and randomly given affix values.
 
Our initial predictors are a set of traits that appears on all items of the weapon class. One important note is that  our initial predictors, the item traits, are not independent of each other as certain affixes are more likely depending on existing affixes and the item type. Thus a Principal Components Analysis is required. This PCA will provide a projection of our data into a space where the predictors are not correlated and thus have the data satisfy one of the assumptions of SVM. We will take the features that explain at least 90% of the variance in the data.

As an aside, the API is fairly limited in that it only provides one method of querying the stashes of users.  Stashes are returned from oldest to most recent postings by user, but a user may appear multiple times if they happen to post multiple items for sale.  For some reason, the API also includes deleted accounts or accounts changed to private after having previously sold items. The API only shows the potential price of an item which is entirely based on how the owner priced the item, but it does not show the price that the item is actually sold at.  The price predictions we end up with ends up skewing towards initial pricing of items rather than actual selling rates.  Some 3rd-party sites that do track item prices and the such spend the time to look through gigabytes of data throughout the entire day.  So while it is feasible to properly sort through the data to retrieve accurate pricing, it would require time and resources that we do not currently have.

One feature that we are using, sockets and links, can be impacted by the type of item that they appear on.  For instance, larger items may have more sockets/links simply due to being larger but does necessarily signify a better/worse item.  The API does not store item size, so we are unable to include that as a feature, but it could potentially influence the results.  Another aspect of sockets is that they can have different color combinations for an item.  Colors signify types of skills that an item can use.  In relation to pricing an item, unusual color combinations can raise the price of an item.  However, in order to tell if the sockets’ colors are “unusual”, we would first have to know the usual colors of the sockets.  As the API does not provide that information, we do not include this feature either, but as with item size, color combination of sockets can also affect the results.
 
For selecting the model parameters (K for KNN, Branch count for Random Forest, and C Linear SVM), we would use cross validation to find the lowest error model, but since we found so few actual usable data points, cross validation would not be useful.  Because users were not consistent in labeling prices on items (might only provide a price during trade or put prices elsewhere), we ended up with a way smaller dataset than we had originally planned.  As mentioned above, in order for us to get the most recent data, we would have to continuously query the API until we reached the end.  If we could obtain more data, we would regularize as well.

The features we start with are rarity of the item, number of sockets, number of links, league of the character, item level, physical damage, critical chance, and attacks per second.  After running PCA, it said that only two features of the original eight are useful.  As stated above, if we were able to work with a larger set of the data, we would be able to have a more accurate representation of the current status of the trade system in PoE.
The KNN regression produced a 0 total error, which it always does when k=1 on the data (training data because our overall data set is too small for a split). The KNN model is probably similar to how players price their items for trade in game, because often times when finding a new item, players search for existing items on trade indexers and price theirs from that. This makes KNN very similar and probably pretty accurate with a large amount of data. Note with higher N, it is probably better to cross validate K and select a higher value since a small K will over fit to noisy prices.
The random forest produced a mean squared error of 74. This model performed the worst of the three, probably because of the very small dataset. With more data, the random forest would likely perform similar to the KNN, as it would divide up the domain into regions of similar feature items, like how KNN does.
And lastly, the SVM has a mean squared error 0.02, which is very good. This is low error likely comes from the size of the data set, and also the kernel. With more data, the pattern would probably not be linear, so if possible we would have used the Gaussian kernel.
One important note, the errors reported are the in sample errors. Since the data set is very small, and obtaining a large data set is too computationally intensive and time consuming given the API, that we collected only 1000 calls of items which ended with only a few data points. This makes a test train split not a good idea, since then the model is weakened even more with a smaller training set. Thus this error is an optimistically biased estimate of out of sample error.
